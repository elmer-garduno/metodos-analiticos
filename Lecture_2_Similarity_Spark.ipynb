{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lecci\u00f3n 2 - Similitud entre conjuntos, escalando con PySpark"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PySpark y similitud entre conjuntos"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Objetivo\n",
      "El objetivo de esta lecci\u00f3n es mostrar como escalar los m\u00e9todos de minhashing aprendidos en la lecci\u00f3n anterior utilizando c\u00f3mputo distribuido sobre Spark."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Dependencias\n",
      "\n",
      "Para esta lecci\u00f3n necesitamos configurar iPython para encontrar las librer\u00edas de PySpark.\n",
      "\n",
      "Editar en archivo `$HOME/.ipython/profile_default/startup/00-pyspark-setup.py`\n",
      "\n",
      "```python\n",
      "# Configure the necessary Spark environment\n",
      "import os\n",
      "os.environ['SPARK_HOME'] = '[path-to-spark-home]/spark/'\n",
      "\n",
      "# And Python path\n",
      "import sys\n",
      "sys.path.insert(0, '[path-to-spark-home]/python')\n",
      "```\n",
      "\n",
      "Descomprimir el archivo `data/wiki-100000.zip` en el directorio `data`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### PySpark\n",
      "\n",
      "Empezemos por entender un poco de Spark. [Spark](http://spark.incubator.apache.org/) es una palataforma de computo distribido que permite hacer an\u00e1lsis de datos de una forma \u00e1gil, tanto para escribrlo como para ejecutarlo.\n",
      "\n",
      "El concepto central en Spark es una colecci\u00f3n distribuida de objetos llamad RDD (Resilient Distributed Dataset), estos pueden ser creados a partir de archivos o a partir de la transformaci\u00f3n de otros RDD.\n",
      "\n",
      "__Nota:__ Se puede utilizar PySpark desde la l\u00ednea de comandos utilizando el comando `pyspark`\n",
      "\n",
      "El primer paso es inicializar el contexto de Spark (este paso no es necesario si se utiliza la consola de PySpark)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "sc = SparkContext(\"local\", \"Minhashing\", pyFiles=['lsh.py'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n leemos un archivo que contiene 100 mil entradas de art\u00edculos en Wikipedia con su clasificaci\u00f3n y mostramos sus primeros 10 renglones.  \n",
      "\n",
      "__Nota:__ Los archivos originales est\u00e1n disponibles en la [DBpedia](http://wiki.dbpedia.org/Downloads38), bajo la licencia [Creative Commons Attribution-ShareAlike License](http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License)  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = sc.textFile(\"data/wiki-100000.txt\")\n",
      "f.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[u'# 2012-06-04T11:00:11Z',\n",
        " u'Autism Autism',\n",
        " u'Autism Communication_disorders',\n",
        " u'Autism Mental_and_behavioural_disorders',\n",
        " u'Autism Neurological_disorders',\n",
        " u'Autism Neurological_disorders_in_children',\n",
        " u'Autism Pervasive_developmental_disorders',\n",
        " u'Autism Psychiatric_diagnosis',\n",
        " u'Autism Learning_disabilities',\n",
        " u'Anarchism Anarchism']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos observar que los renglones continen dos partes: el nombre del art\u00edculo y la categor\u00eda a la que pertencen. A continuaci\u00f3n dividimos cada rengl\u00f3n en sus componentes utilizando operaciones estandar sobre un RDD, en este caso `map` y `filter`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filtered = f.filter(lambda line: not line.startswith('#'))\n",
      "split = filtered.map(lambda line: line.split(\" \"))\n",
      "print split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<pyspark.rdd.PipelinedRDD object at 0x10357da10>\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Este segmento de c\u00f3digo nos muestra varias cosas interesantes, la primera es que estamos pasando funciones `lambda` a los m\u00e9todos, para que sean aplicadas a cada rengl\u00f3n del `RDD`. \n",
      "\n",
      "La segunda es que al aplicar los m\u00e9todos sobre un `RDD` obtenemos de nuevo una estructura `RDD`. En Spark, la mayor\u00eda de las funciones sobre un `RDD` no ejecutan el contenido sino que lo agregan al grafo de ejecuci\u00f3n, y solamente cuando se llaman a funciones como `take` o `collect` es que se ejectua dicho grafo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[[u'Autism', u'Autism'],\n",
        " [u'Autism', u'Communication_disorders'],\n",
        " [u'Autism', u'Mental_and_behavioural_disorders'],\n",
        " [u'Autism', u'Neurological_disorders'],\n",
        " [u'Autism', u'Neurological_disorders_in_children'],\n",
        " [u'Autism', u'Pervasive_developmental_disorders'],\n",
        " [u'Autism', u'Psychiatric_diagnosis'],\n",
        " [u'Autism', u'Learning_disabilities'],\n",
        " [u'Anarchism', u'Anarchism'],\n",
        " [u'Anarchism', u'Political_culture']]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n asociaremos a cada art\u00edculo las categor\u00edas a las que pertence, utilizando la funci\u00f3n `groupByKey`, en este caso se considera el primer elemento de cada rengl\u00f3n como la llave."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = split.groupByKey()\n",
      "articles.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[(u'Rage_Hard',\n",
        "  [u'Frankie_Goes_to_Hollywood_songs',\n",
        "   u'1986_singles',\n",
        "   u'Number-one_singles_in_Germany',\n",
        "   u'Songs_produced_by_Stephen_Lipson']),\n",
        " (u'Diogo_C%C3%A3o',\n",
        "  [u'1450s_births',\n",
        "   u'15th-century_deaths',\n",
        "   u'People_from_Vila_Real_Municipality',\n",
        "   u'Portuguese_explorers',\n",
        "   u'Explorers_of_Africa',\n",
        "   u'15th_century_in_Africa',\n",
        "   u'15th-century_explorers',\n",
        "   u'Navigators',\n",
        "   u'Maritime_history_of_Portugal',\n",
        "   u'15th-century_Portuguese_people',\n",
        "   u'Age_of_Discovery']),\n",
        " (u'Dhrystone', [u'Computer_benchmarks', u'1984_introductions']),\n",
        " (u'Dubbing_(filmmaking)', [u'Dubbing_(filmmaking)']),\n",
        " (u'Szczecin',\n",
        "  [u'Szczecin',\n",
        "   u'Port_cities_and_towns_in_Poland',\n",
        "   u'Port_cities_and_towns_of_the_Baltic_Sea',\n",
        "   u'City_counties_of_Poland',\n",
        "   u'Cities_and_towns_in_West_Pomeranian_Voivodeship'])]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El ejemplo anterior nos muestra algunas de las operaciones b\u00e1sicas que se puden realizat con Spark. En la siguiente secci\u00f3n vamos a retomar el ejemplo de _Min-hashing_ y aplicarlo para encontrar art\u00edculos similares."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Similitud entre conjuntos, modelo distribuido"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aplicando el modelo LSH aprendido en la lecci\u00f3n anterior, aplicamos la funci\u00f3n lsh a cada uno de los conjuntos que definen los temas de cada art\u00edculo. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lsh import LSH, get_conf\n",
      "\n",
      "conf = get_conf(0.90)\n",
      "\n",
      "lsh = articles.map(lambda _: (_[0], LSH(conf).lsh(_[1])))\n",
      "lsh.take(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'Rage_Hard',\n",
        "  [1698531787L,\n",
        "   674338419L,\n",
        "   294784065L,\n",
        "   2132353950L,\n",
        "   2922530270L,\n",
        "   3092529889L]),\n",
        " (u'Diogo_C%C3%A3o',\n",
        "  [3652685019L,\n",
        "   1510720539L,\n",
        "   973807287L,\n",
        "   2397071194L,\n",
        "   2239725734L,\n",
        "   1219044189L])]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La clase `LSH` y el m\u00e9todo `get_conf` est\u00e1n definidos en el archivo `lsh.py`. \n",
      "\n",
      "Para probar la aplicaci\u00f3n de este m\u00e9todo creamos un mapa con las firmas a las firmas asociadas a cada art\u00edculo. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def entry(data):\n",
      "    id = data[0]\n",
      "    return [(sig, id) for sig in data[1]]\n",
      "\n",
      "signatures = lsh.flatMap(lambda _: entry(_)).groupByKey()\\\n",
      "    .filter(lambda _: len(_[1]) > 1).cache()\n",
      "signatures.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(2182873143L, [u'Intercalation', u'Calendar', u'Calendar_date']),\n",
        " (3949461521L, [u'Allophone', u'Phone_(phonetics)']),\n",
        " (1846891891L, [u\"Finagle's_law\", u'Laws_of_infernal_dynamics']),\n",
        " (269222078L,\n",
        "  [u'Autrefois_acquit', u'Autrefois_convict', u'Pardon_(plea)', u'Plea']),\n",
        " (1919156655L, [u'Colour', u'Roman_Catholic', u'Weak_force', u'Beatles'])]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este ejemplo es posible observar como art\u00edculos que contienen las mismas categor\u00edas se agrupan en una sola cubeta. A continuaci\u00f3n obtenemos el conjunto de art\u00edculos similares a un art\u00edculo dado a partir del valor de sus firmas."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_90 = signatures.collectAsMap()\n",
      "index = articles.collectAsMap()\n",
      "\n",
      "def similar(doc, db, conf):\n",
      "    hashes = LSH(conf).lsh(doc)\n",
      "    candidates = set()\n",
      "    for sig in hashes:\n",
      "        candidates.update(set(db[sig]))\n",
      "    for sim in candidates:\n",
      "        print '%s -> %s' % (sim, index[sim])\n",
      "\n",
      "doc = index['Third_Epistle_of_John']\n",
      "similar(doc, db_90, conf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_lsh(threshold, dataset):\n",
      "    conf_s = get_conf(threshold)\n",
      "    lsh_s = dataset.map(lambda _: (_[0], LSH(conf_s).lsh(_[1])))\n",
      "    db_s = lsh_s.flatMap(lambda _: entry(_)).groupByKey()\\\n",
      "        .filter(lambda _: len(_[1]) > 1).collectAsMap()\n",
      "    return (conf_s, db_s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = index['Third_Epistle_of_John']\n",
      "conf_s70, db_s70 = create_lsh(0.7, articles)\n",
      "similar(doc, db_s70, conf_s70)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epistle_of_James -> [u'New_Testament_books', u'Canonical_epistles']\n",
        "Second_Epistle_of_John -> [u'New_Testament_books', u'Anti-Gnosticism', u'Canonical_epistles', u'Johannine_literature']\n",
        "Epistle_of_Jude -> [u'New_Testament_books', u'Canonical_epistles']\n",
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = index['Third_Epistle_of_John']\n",
      "conf_s40, db_s40 = create_lsh(0.4, articles)\n",
      "similar(doc, db_s40, conf_s40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Second_Epistle_of_John -> [u'New_Testament_books', u'Anti-Gnosticism', u'Canonical_epistles', u'Johannine_literature']\n",
        "Epistle_to_the_Romans -> [u'50s_books', u'Canonical_epistles', u'New_Testament_books', u'Pauline-related_books', u'Epistle_to_the_Romans']\n",
        "First_Epistle_of_Peter -> [u'New_Testament_books', u'Petrine-related_books', u'Canonical_epistles']\n",
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Second_Epistle_of_Peter -> [u'New_Testament_books', u'Petrine-related_books', u'Canonical_epistles']\n",
        "Epistle_of_James -> [u'New_Testament_books', u'Canonical_epistles']\n",
        "Epistle_of_Jude -> [u'New_Testament_books', u'Canonical_epistles']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sys.getsizeof(db_90), sys.getsizeof(db_s70), sys.getsizeof(db_s40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "196888 196888 786712\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n comparamos la precisi\u00f3n de el m\u00e9todo de LSH con el m\u00e9todo directo utilizando similitud de Jaccard."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alt = split.map(lambda _: (_[1], _[0])).groupByKey().collectAsMap()\n",
      "\n",
      "from lsh import jaccard\n",
      "\n",
      "def alt_similar(doc, db, threshold):\n",
      "    candidates = set()\n",
      "    for cat in doc:\n",
      "        candidates.update(set(db[cat]))\n",
      "    for sim in candidates:\n",
      "        if jaccard(set(doc), set(index[sim])) > threshold:\n",
      "            print '%s -> %s' % (sim, index[sim])\n",
      "\n",
      "sys.getsizeof(alt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "3146008"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = index['Third_Epistle_of_John']\n",
      "alt_similar(doc, alt, 0.9)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = index['Third_Epistle_of_John']\n",
      "alt_similar(doc, alt, 0.7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Second_Epistle_of_John -> [u'New_Testament_books', u'Anti-Gnosticism', u'Canonical_epistles', u'Johannine_literature']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = index['Third_Epistle_of_John']\n",
      "alt_similar(doc, alt, 0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First_Epistle_of_Peter -> [u'New_Testament_books', u'Petrine-related_books', u'Canonical_epistles']\n",
        "Third_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Epistle_of_James -> [u'New_Testament_books', u'Canonical_epistles']\n",
        "First_Epistle_of_John -> [u'New_Testament_books', u'Canonical_epistles', u'Johannine_literature']\n",
        "Epistle_of_Jude -> [u'New_Testament_books', u'Canonical_epistles']\n",
        "Second_Epistle_of_John -> [u'New_Testament_books', u'Anti-Gnosticism', u'Canonical_epistles', u'Johannine_literature']\n",
        "Second_Epistle_of_Peter -> [u'New_Testament_books', u'Petrine-related_books', u'Canonical_epistles']\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos observar en los resultados que el m\u00e9todo LSH efectivamente genero falsos positivos que estaban por arriba del umbral de similitud."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}